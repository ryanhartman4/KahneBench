# Handoff Document
Generated: 2026-02-09

## Summary

Integrating 5 methodological innovations from Anthropic's [BLOOM auto-eval framework](https://github.com/safety-research/bloom) into KahneBench, the cognitive bias benchmark for LLMs. The work was parallelized using an agent team (`bloom-integration`) with 5 teammates building independent modules simultaneously, and a lead responsible for integrating them into shared files.

**Plan file:** `~/.claude/plans/snappy-dancing-starlight.md` (full spec with all 5 features)

## Current Status

### Completed (5 new modules + tests, 464/464 tests passing)

| Feature | Source File | Test File | Tests | Status |
|---------|-----------|-----------|-------|--------|
| 1. LLM-as-Judge Fallback | `engines/judge.py` | `tests/test_judge.py` | 19 pass | Done |
| 2. Variation Dimensions | `engines/variation.py` | `tests/test_variation.py` | 20 pass | Done |
| 3. Quality Meta-Metrics | `engines/quality.py` | `tests/test_quality.py` | 16 pass | Done |
| 4. BLOOM Scenario Generation | `engines/bloom_generator.py` | `tests/test_bloom_generator.py` | 29 pass | **Tests broken** |
| 5. Multi-Turn Evaluation | `engines/conversation.py` | `tests/test_conversation.py` | 19 pass | Done |

### In Progress / Blocked

**Task #6: Lead Integration into Shared Files** — NOT YET STARTED. This is the critical remaining work.

### Feature 4 Test File Issue

`bloom_generator.py` was correctly rewritten to the BLOOM LLM-driven implementation (Understanding -> Ideation pipeline using `LLMProvider`), but `test_bloom_generator.py` still contains tests from the **old incorrect implementation** (Bloom's Taxonomy version). The test file currently passes (29 tests) because linter auto-fixed the source back to old version before the new version was written. **This test file needs to be completely rewritten** to test the async LLM-driven generator. A corrected version was drafted but not yet saved due to a file locking conflict. See the "Next Steps" section.

## Recent Changes

All files below are **untracked** (not yet committed):

### New Source Files (in `src/kahne_bench/engines/`)
- **`judge.py`** — `LLMJudge` class with `JudgeResult` dataclass, `JUDGE_SCORING_PROMPT` template, XML parsing. Provides fallback scoring when regex extraction returns None.
- **`variation.py`** — `VariationDimension` enum (6 dimensions: emotional_pressure, authority_framing, numeric_noise, social_pressure, time_pressure, stakes_escalation), `VariationGenerator`, `VariationRobustnessScore`.
- **`quality.py`** — `TestQualityJudge` with `TestQualityScores` and `TestQualityReport`. Assesses test case realism (1-10), elicitation difficulty (1-10), detection awareness (1-10). **Note:** Dataclass names start with `Test` which causes pytest collection warnings — rename to avoid `Test` prefix.
- **`bloom_generator.py`** — **Currently reverted to wrong implementation (Bloom's Taxonomy).** Needs to be the BLOOM LLM-driven version per the plan. The correct source was written but auto-reverted.
- **`conversation.py`** — `ConversationalEvaluator` with `ConversationTranscript`, `ConversationStrategy` (PROBE/CHALLENGE/REINFORCE/NEUTRAL), `ConversationalBiasScore` metric. Uses orchestrator LLM as "user" probing target model across turns.

### New Test Files (in `tests/`)
- `test_judge.py` (19 tests)
- `test_variation.py` (20 tests)
- `test_quality.py` (16 tests)
- `test_bloom_generator.py` (29 tests — **needs rewrite**, see above)
- `test_conversation.py` (19 tests)

## Next Steps

### Immediate Priority: Fix Feature 4

1. **Rewrite `bloom_generator.py`** to the correct BLOOM LLM-driven version:
   - Must have `provider: LLMProvider` parameter (async LLM calls)
   - `understand_bias(bias_def: BiasDefinition) -> BiasUnderstanding` — Stage 1
   - `generate_scenarios(understanding, bias_def, domain) -> list[GeneratedScenario]` — Stage 2
   - `scenario_to_instance(scenario, bias_def) -> CognitiveBiasInstance` — Conversion
   - XML parsing for `<behavioral_markers>`, `<trigger_patterns>`, `<scenario>` tags
   - The correct implementation exists in the plan file — just needs to be applied

2. **Rewrite `test_bloom_generator.py`** with async tests using `MockLLMProvider` that returns XML responses. A complete corrected version was drafted in conversation (search for `MOCK_UNDERSTANDING_RESPONSE` and `MOCK_IDEATION_RESPONSE` in the conversation).

### Task #6: Integrate into Shared Files

After Feature 4 is fixed, integrate all 5 modules into the 4 shared files:

#### `engines/evaluator.py`
- Add `judge: LLMJudge | None = None` param to `BiasEvaluator.__init__()` (line 634)
- In `run_single_trial()` (line 722), after scoring returns `(None, None)` at line 764:
  - If `self.judge is not None`, call `await self.judge.score(...)`
  - Set `result.metadata["scoring_method"]` to `"llm_judge"` or `"regex"`

#### `cli.py`
- Add `--judge-provider` / `--judge-model` flags to `evaluate` command (line 281)
- Add `--variations` flag to `evaluate` command
- New commands: `assess-quality`, `generate-bloom`, `evaluate-conversation`

#### `metrics/core.py`
- Add `VariationRobustnessScore` import/integration
- Add `ConversationalBiasScore` import/integration
- Extend `CognitiveFingerprintReport` (line 802) with optional fields:
  - `variation_robustness: dict[str, VariationRobustnessScore]`
  - `conversational_scores: dict[str, ConversationalBiasScore]`
  - `test_quality: TestQualityReport | None`

#### `utils/io.py`
- Add `export_quality_report_to_json()`
- Add `export_transcripts_to_json()`
- Add serialization for new metric types

### Final Steps
- Run full test suite: `PYTHONPATH=src uv run pytest -v`
- Integration smoke test with mock provider
- Commit all changes

## Context & Notes

### Architecture Decision: Fan-out/Fan-in
Each new module was built as a standalone file with its own `LLMProvider` Protocol definition (to avoid circular imports). During integration, the evaluator.py `LLMProvider` should be the canonical one — consider having the new modules import from evaluator.py or extracting the protocol to a shared location.

### Quality.py Naming Issue
The dataclasses `TestQualityScores`, `TestQualityReport`, `TestQualityJudge` trigger pytest collection warnings because they start with `Test`. Rename to `QualityScores`, `QualityReport`, `QualityJudge` to fix.

### Key Design Decisions
- **Judge is fallback only** (user choice): Only runs when regex extraction returns None. No double-scoring.
- **Variation dimensions are template-based** (prefix/suffix wrappers) — not LLM-generated. Fast and deterministic.
- **BLOOM generation requires an LLM provider** — cannot run without API access. Tests use MockLLMProvider.
- **Conversation evaluator needs TWO providers** — one for target model, one for orchestrator playing "user".
- **All new metrics are optional fields** on `CognitiveFingerprintReport` — backward compatible.

### Agent Team Cleanup
The `bloom-integration` team is still active at `~/.claude/teams/bloom-integration/`. Some teammates may still be running. Clean up with:
```bash
# After shutting down all teammates:
# TeamDelete (via Claude Code) or manually rm -rf ~/.claude/teams/bloom-integration ~/.claude/tasks/bloom-integration
```
